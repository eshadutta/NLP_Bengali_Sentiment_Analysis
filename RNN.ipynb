{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis through Metric Learning\n",
    "\n",
    "In this project, we are utilizing the \n",
    "\n",
    "\"@inproceedings{sazzed2019sentiment, title={A Sentiment Classification in Bengali and Machine Translated English Corpus}, author={Sazzed, Salim and Jayarathna, Sampath}, booktitle={2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science (IRI)}, pages={107--114}, year={2019}, organization={IEEE} }\"\n",
    "\n",
    "data to implement sentiment analysis through siamese network. \n",
    "\n",
    "There are in total 3307 negative comments and 8500 positive comments present\n",
    "\n",
    "We will implement Metric Learning Techniques to proceed with the analysis. The Siamese Neural Network architecture would help us to achieve the implementation. The network would have two parts. One for distance learning, the other for sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files needed to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from itertools import combinations\n",
    "import random\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None \n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from bnlp import NLTKTokenizer\n",
    "from bnlp.corpus import stopwords, punctuations\n",
    "from bnlp.corpus.util import remove_stopwords\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_file = open('data/all_positive_8500.txt', encoding=\"utf8\")\n",
    "pos_lines = pos_file.readlines()\n",
    "pos_values = [1]*len(pos_lines)\n",
    "\n",
    "neg_file = open('data/all_negative_3307.txt', encoding=\"utf8\")\n",
    "neg_lines = neg_file.readlines()\n",
    "neg_values = [-1]*len(neg_lines)\n",
    "\n",
    "X = np.array(pos_lines + neg_lines)\n",
    "y = np.array(pos_values + neg_values)\n",
    "\n",
    "#removing whitespaces, punctuations, digits and english words from text\n",
    "converted_X =[]\n",
    "punctuation_list = ['[',',','-','_','=',':','+','$','@',\n",
    "                    '~','!',';','/','^',']','{','}','(',')','<','>','.']\n",
    "whitespace = re.compile(u\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\n",
    "bangla_digits = u\"[\\u09E6\\u09E7\\u09E8\\u09E9\\u09EA\\u09EB\\u09EC\\u09ED\\u09EE\\u09EF]+\"\n",
    "english_chars = u\"[a-zA-Z0-9]\"\n",
    "punc = u\"[(),$%^&*+={}\\[\\]:\\\"|\\'\\~`<>/,¦!?½£¶¼©⅐⅑⅒⅓⅔⅕⅖⅗⅘⅙⅚⅛⅜⅝⅞⅟↉¤¿º;-]+\"\n",
    "bangla_fullstop = u\"\\u0964\"     #bangla fullstop(dari)\n",
    "punctSeq   = u\"['\\\"“”‘’]+|[.?!,…]+|[:;]+\"\n",
    "\n",
    "\n",
    "for x in X:\n",
    "    x = re.sub(bangla_digits, \" \", x)\n",
    "    x = re.sub(punc, \" \", x)\n",
    "    x = re.sub(english_chars, \" \", x)\n",
    "    x = re.sub(bangla_fullstop, \" \", x)\n",
    "    x = re.sub(punctSeq, \" \", x)\n",
    "    x = whitespace.sub(\" \", x).strip()\n",
    "    \n",
    "    x = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE)\n",
    "    x = re.sub(r'\\<a href', ' ', x)\n",
    "    x = re.sub(r'&amp;‘:‘ ’', '', x) \n",
    "    x = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]। ,', ' ', x)\n",
    "    x = re.sub(r'<br />', ' ', x)\n",
    "    x = re.sub(r'\\'', ' ', x)\n",
    "    x = re.sub(r\"[\\@$#%~+-\\.\\'।\\\"]\",\" \",x)\n",
    "    x = re.sub(r\"(?m)^\\s+\", \"\", x)\n",
    "    x = re.sub(\"[()]\",\"\",x)\n",
    "    x = re.sub(\"[‘’]\",\"\",x)\n",
    "    x = re.sub(\"[!]\",\"\",x)\n",
    "    x = re.sub(\"[/]\",\"\",x)\n",
    "    x = re.sub(\"[:]\",\"\",x)\n",
    "    x = re.sub('\\ |\\?|\\.|\\!|\\/|\\;|\\:', ' ',x)\n",
    "    x = x.strip(\"/\")\n",
    "    converted_X.append(x)\n",
    "converted_X = np.array(converted_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Sets and Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, random_state = 42)\n",
    "fold_count = 0\n",
    "max_features = 2500\n",
    "embedding_dim = 64\n",
    "batch_size = 32\n",
    "macro_avg = {'precision':[],'recall':[],'f1-score':[]}\n",
    "weighted_avg =  {'precision':[],'recall':[],'f1-score':[]}\n",
    "metrics = ['precision', 'recall', 'f1-score']\n",
    "epochs = 500\n",
    "\n",
    "for train_index, test_index in skf.split(converted_X, y):\n",
    "    \n",
    "    X_train, X_test = converted_X[train_index], converted_X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    \n",
    "    X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42) #approximate train, test, eval size (8500,) (1181,) (2126,)\n",
    "    \n",
    "    #Data processing: Tokenization\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(X_train) #fitting the tokenizer on the train data\n",
    "    X_train_tokenized_padded = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(X_train))\n",
    "    X_test_tokenized_padded = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(X_test))\n",
    "    X_eval_tokenized_padded = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(X_eval))\n",
    "    \n",
    "    #sample weights for training\n",
    "    y_orig_combined = np.array(y_train).reshape(-1,)\n",
    "    class_weights_combined = compute_class_weight('balanced',[-1,1], y_orig_combined)\n",
    "\n",
    "    #making sample weight array\n",
    "    y_orig_combined = np.where(y_orig_combined==-1, class_weights_combined[0], y_orig_combined)\n",
    "    y_orig_combined = np.where(y_orig_combined==1, class_weights_combined[1], y_orig_combined)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #the RNN model\n",
    "    \n",
    "    rnn_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(\n",
    "            input_dim=max_features,\n",
    "            output_dim=embedding_dim,\n",
    "            # Use masking to handle the variable sequence lengths\n",
    "            mask_zero=True),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    rnn_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "     \n",
    "    es = tf.keras.callbacks.EarlyStopping(patience=3)\n",
    "    \n",
    "    checkpoint_path = \"saved_model/RNN/\"+str(fold_count)+\"/cp.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    # Create a callback that saves the model's weights\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=1)\n",
    "\n",
    "    hist = rnn_model.fit(x=[X_train_tokenized_padded], y=[y_train], \n",
    "                        validation_data = ([X_eval_tokenized_padded], [y_eval]) ,\n",
    "                        sample_weight = y_orig_combined, batch_size=batch_size, epochs=epochs, verbose=0, callbacks=[es,cp_callback])\n",
    "    predictions = tf.keras.activations.sigmoid(rnn_model.predict(X_test_tokenized_padded))\n",
    "    predictions = np.where(predictions>0.5, 1, -1)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    cl = classification_report(y_test, predictions, output_dict =True)\n",
    "    for t in metrics:\n",
    "        macro_avg[t].append(cl['macro avg'][t])\n",
    "        weighted_avg[t].append(cl['weighted avg'][t])\n",
    "    \n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    fold_count += 1\n",
    "    \n",
    "print(macro_avg, weighted_avg)\n",
    "    \n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
