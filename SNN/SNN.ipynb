{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis through Metric Learning\n",
    "\n",
    "In this project, we are utilizing the \n",
    "\n",
    "\"@inproceedings{sazzed2019sentiment, title={A Sentiment Classification in Bengali and Machine Translated English Corpus}, author={Sazzed, Salim and Jayarathna, Sampath}, booktitle={2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science (IRI)}, pages={107--114}, year={2019}, organization={IEEE} }\"\n",
    "\n",
    "data to implement sentiment analysis through siamese network. \n",
    "\n",
    "There are in total 3307 negative comments and 8500 positive comments present\n",
    "\n",
    "We will implement Metric Learning Techniques to proceed with the analysis. The Siamese Neural Network architecture would help us to achieve the implementation. The network would have two parts. One for distance learning, the other for sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files needed to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from itertools import combinations\n",
    "import random\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None \n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_generator(X, y):\n",
    "    #equal number of similar and dissimilar pair geneartor\n",
    "    x1 = np.zeros((int(np.ceil(X.shape[0]/2))*2, X.shape[1],))\n",
    "    x2 = np.zeros((int(np.ceil(X.shape[0]/2))*2, X.shape[1],))\n",
    "    y1_label =  np.zeros((int(np.ceil(X.shape[0]/2))*2, 1,))\n",
    "    y2_label =  np.zeros((int(np.ceil(X.shape[0]/2))*2, 1,))\n",
    "    y_simil =  np.zeros((int(np.ceil(X.shape[0]/2))*2, 1,))\n",
    "    \n",
    "    marker_comb = list(combinations(list(range(X.shape[0])),2))\n",
    "    random.shuffle(marker_comb)\n",
    "    \n",
    "    simil_count = 0\n",
    "    disimil_count = 0\n",
    "    count = 0\n",
    "    fill_up_count = 0\n",
    "    \n",
    "    \n",
    "    while count<len(marker_comb):\n",
    "        \n",
    "        ids = marker_comb[count]\n",
    "        \n",
    "        one_val, one_label = X[ids[0]], y[ids[0]]\n",
    "        two_val, two_label = X[ids[1]], y[ids[1]]\n",
    "        \n",
    "        if one_label == two_label:\n",
    "            if simil_count<int(np.ceil(X.shape[0]/2)):\n",
    "                x1[fill_up_count] = one_val\n",
    "                x2[fill_up_count] = two_val\n",
    "                y1_label[fill_up_count] = one_label\n",
    "                y2_label[fill_up_count] = two_label\n",
    "                \n",
    "                simil_count +=1\n",
    "                y_simil[fill_up_count] = 1 #similar labels\n",
    "                fill_up_count += 1\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            \n",
    "            if disimil_count<int(np.ceil(X.shape[0]/2)):\n",
    "                x1[fill_up_count] = one_val\n",
    "                x2[fill_up_count] = two_val\n",
    "                y1_label[fill_up_count] = one_label\n",
    "                y2_label[fill_up_count] = two_label\n",
    "                \n",
    "                disimil_count += 1\n",
    "                y_simil[fill_up_count] = -1 #dissimilar labels\n",
    "                fill_up_count += 1\n",
    "        count +=1\n",
    "    x1 = x1[~np.all(y1_label == 0, axis=1)]\n",
    "    x2 = x2[~np.all(y1_label == 0, axis=1)]\n",
    "    y1_label = y1_label[~np.all(y1_label == 0, axis=1)]\n",
    "    y2_label = y2_label[~np.all(y1_label == 0, axis=1)]\n",
    "    y_simil = y_simil[~np.all(y1_label == 0, axis=1)]\n",
    "    y_simil = np.array(y_simil, dtype = np.float32)\n",
    "    \n",
    "    print(count, simil_count, disimil_count)\n",
    "    return x1, x2, y1_label, y2_label, y_simil\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_model(max_len, max_features,embedding_dim):\n",
    "    #Here the siamese network part is defined\n",
    "    input = tf.keras.Input(shape = (max_len,), name = 'Input')\n",
    "    x = input\n",
    "    x = tf.keras.layers.Embedding(\n",
    "            input_dim=max_features,\n",
    "            output_dim=embedding_dim,\n",
    "            # Use masking to handle the variable sequence lengths\n",
    "            mask_zero=True)(x)\n",
    "    x = tf.keras.layers.LSTM(64)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "  \n",
    "    return tf.keras.Model(inputs = input, outputs = x)\n",
    "    \n",
    "\n",
    "def classification_model():\n",
    "    #Here the classification network part is defined\n",
    "    input = tf.keras.Input(shape = (64,))\n",
    "    x = input\n",
    "    x = tf.keras.layers.Dense(units = 1,\n",
    "                             name = 'Classification_output')(x) \n",
    "    return tf.keras.Model(inputs = input, outputs = x)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    #contrastive loss\n",
    "    print(y_true.shape, y_pred.shape)\n",
    "    margin = 1\n",
    "    square_pred = tf.math.square(y_pred)\n",
    "    margin_square = tf.math.square(tf.math.maximum(margin - y_pred, 0))\n",
    "    return tf.keras.backend.mean(y_true* square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "def cross_entropy_loss_defined(y_true, y_pred):\n",
    "    return tf.keras.losses.BinaryCrossentropy(from_logits=True)(y_true, y_pred)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis = 1, keepdims=True)\n",
    "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def similarity_accuracy(y_true, y_pred):\n",
    "    return tf.keras.backend.mean(tf.math.equal(y_true, tf.cast(y_pred< 0.5, y_true.dtype))) #it is less than .5 as the similar paris are 1\n",
    "\n",
    "def non_nan_average(x):\n",
    "    # Computes the average of all elements that are not NaN in a rank 1 tensor\n",
    "    nan_mask = tf.math.is_nan(x)\n",
    "    x = tf.boolean_mask(x, tf.logical_not(nan_mask))\n",
    "    return tf.keras.backend.mean(x)\n",
    "    \n",
    "\n",
    "def class_accuracy(y_true, y_pred): \n",
    "    \n",
    "    y_pred = tf.keras.activations.sigmoid(y_pred)\n",
    "    y_pred = tf.where(y_pred>0.5, 1, -1)\n",
    "    confusion_matrix = tf.math.confusion_matrix(y_true, y_pred)\n",
    "    total_instance = tf.reduce_sum(confusion_matrix, axis = 1)\n",
    "    correct_instances = tf.linalg.tensor_diag_part(confusion_matrix)\n",
    "    ratio = tf.divide(correct_instances, tf.maximum(1,total_instance))\n",
    "    uar = non_nan_average(ratio)   \n",
    "    \n",
    "    return uar\n",
    "    \n",
    "\n",
    "\n",
    "def total_model(max_len, max_features,embedding_dim):\n",
    "    siamese_network = siamese_model(max_len, max_features,embedding_dim)\n",
    "    \n",
    "    input_a = tf.keras.Input(shape = max_len)\n",
    "    input_b = tf.keras.Input(shape = max_len)\n",
    "    \n",
    "    processed_a = siamese_network(input_a)\n",
    "    processed_b = siamese_network(input_b)\n",
    "    \n",
    "    distance = tf.keras.layers.Lambda(euclidean_distance, \n",
    "                                      output_shape = eucl_dist_output_shape, \n",
    "                                      name = 'Distance' )([processed_a, processed_b])\n",
    "    \n",
    "    classification_network = classification_model()\n",
    "    \n",
    "    accuracy_a = classification_network(processed_a)\n",
    "    accuracyoutputa = tf.keras.layers.Lambda(lambda x: x, name = 'accuracyoutput_a')([accuracy_a])\n",
    "    \n",
    "    accuracy_b = classification_network(processed_b)\n",
    "    accuracyoutputb = tf.keras.layers.Lambda(lambda x: x, name = 'accuracyoutput_b')([accuracy_b])\n",
    "    \n",
    "    return tf.keras.Model(inputs = [input_a, input_b],outputs = [distance, accuracyoutputa, accuracyoutputb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_file = open('data/all_positive_8500.txt', encoding=\"utf8\")\n",
    "pos_lines = pos_file.readlines()\n",
    "pos_values = [1]*len(pos_lines)\n",
    "\n",
    "neg_file = open('data/all_negative_3307.txt', encoding=\"utf8\")\n",
    "neg_lines = neg_file.readlines()\n",
    "neg_values = [-1]*len(neg_lines)\n",
    "\n",
    "X = np.array(pos_lines + neg_lines)\n",
    "y = np.array(pos_values + neg_values)\n",
    "\n",
    "#removing whitespaces, punctuations, digits and english words from text\n",
    "converted_X =[]\n",
    "punctuation_list = ['[',',','-','_','=',':','+','$','@',\n",
    "                    '~','!',';','/','^',']','{','}','(',')','<','>','.']\n",
    "whitespace = re.compile(u\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\n",
    "bangla_digits = u\"[\\u09E6\\u09E7\\u09E8\\u09E9\\u09EA\\u09EB\\u09EC\\u09ED\\u09EE\\u09EF]+\"\n",
    "english_chars = u\"[a-zA-Z0-9]\"\n",
    "punc = u\"[(),$%^&*+={}\\[\\]:\\\"|\\'\\~`<>/,¦!?½£¶¼©⅐⅑⅒⅓⅔⅕⅖⅗⅘⅙⅚⅛⅜⅝⅞⅟↉¤¿º;-]+\"\n",
    "bangla_fullstop = u\"\\u0964\"     #bangla fullstop(dari)\n",
    "punctSeq   = u\"['\\\"“”‘’]+|[.?!,…]+|[:;]+\"\n",
    "\n",
    "\n",
    "for x in X:\n",
    "    x = re.sub(bangla_digits, \" \", x)\n",
    "    x = re.sub(punc, \" \", x)\n",
    "    x = re.sub(english_chars, \" \", x)\n",
    "    x = re.sub(bangla_fullstop, \" \", x)\n",
    "    x = re.sub(punctSeq, \" \", x)\n",
    "    x = whitespace.sub(\" \", x).strip()\n",
    "    \n",
    "    x = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE)\n",
    "    x = re.sub(r'\\<a href', ' ', x)\n",
    "    x = re.sub(r'&amp;‘:‘ ’', '', x) \n",
    "    x = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]। ,', ' ', x)\n",
    "    x = re.sub(r'<br />', ' ', x)\n",
    "    x = re.sub(r'\\'', ' ', x)\n",
    "    x = re.sub(r\"[\\@$#%~+-\\.\\'।\\\"]\",\" \",x)\n",
    "    x = re.sub(r\"(?m)^\\s+\", \"\", x)\n",
    "    x = re.sub(\"[()]\",\"\",x)\n",
    "    x = re.sub(\"[‘’]\",\"\",x)\n",
    "    x = re.sub(\"[!]\",\"\",x)\n",
    "    x = re.sub(\"[/]\",\"\",x)\n",
    "    x = re.sub(\"[:]\",\"\",x)\n",
    "    x = re.sub('\\ |\\?|\\.|\\!|\\/|\\;|\\:', ' ',x)\n",
    "    x = x.strip(\"/\")\n",
    "    converted_X.append(x)\n",
    "converted_X = np.array(converted_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Sets and Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, random_state = 42)\n",
    "fold_count = 0\n",
    "max_features = 2500\n",
    "embedding_dim = 64\n",
    "batch_size = 256\n",
    "macro_avg = {'precision':[],'recall':[],'f1-score':[]}\n",
    "weighted_avg =  {'precision':[],'recall':[],'f1-score':[]}\n",
    "metrics = ['precision', 'recall', 'f1-score']\n",
    "epochs = 1\n",
    "read_from_store_data = 1\n",
    "maxlen = 250\n",
    "\n",
    "if read_from_store_data:\n",
    "    for fc in range(10):\n",
    "        with open('train_eval_test_data_'+str(fold_count)+'.pkl','rb') as f:\n",
    "            X1_train, X2_train, y1_label_train, y2_label_train, y_simil_train, X1_eval, X2_eval, y1_label_eval, y2_label_eval, y_simil_eval, X_test_tokenized_padded, y_test = pickle.load(f)\n",
    "        final_model = total_model(maxlen, max_features,embedding_dim)\n",
    "        losses = {'Distance': contrastive_loss, 'accuracyoutput_a': cross_entropy_loss_defined, 'accuracyoutput_b': cross_entropy_loss_defined}\n",
    "    #     weights = {'Distance': distance_weight, 'accuracyoutput':class_weight}\n",
    "        metrices = {'Distance':similarity_accuracy, 'accuracyoutput_a': class_accuracy, 'accuracyoutput_b': class_accuracy}\n",
    "        final_model.compile(optimizer = tf.keras.optimizers.Adam(1e-4), loss = losses)#, metrics = metrices)\n",
    "        print(final_model.summary())\n",
    "\n",
    "        checkpoint_path = \"saved_model/SNN/\"+str(fold_count)+\"/cp.ckpt\"\n",
    "        checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "        # Create a callback that saves the model's weights\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         verbose=1)\n",
    "\n",
    "        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=3)\n",
    "        final_model.fit(x = [X1_train, X2_train], y = [y_simil_train, y1_label_train, y2_label_train], batch_size=batch_size, epochs= epochs,\n",
    "                       validation_data = ([X1_eval, X2_eval], [y_simil_eval, y1_label_eval, y2_label_eval]), verbose = 1, callbacks=[es, cp_callback])\n",
    "        predictions =  tf.keras.activations.sigmoid(final_model.predict([X_test_tokenized_padded, X_test_tokenized_padded])[-1])\n",
    "        \n",
    "        predictions = np.where(predictions>0.5, 1, -1)\n",
    "        print(classification_report(y_test, predictions))\n",
    "        cl = classification_report(y_test, predictions, output_dict =True)\n",
    "        for t in metrics:\n",
    "            macro_avg[t].append(cl['macro avg'][t])\n",
    "            weighted_avg[t].append(cl['weighted avg'][t])\n",
    "\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        fold_count += 1\n",
    "else:\n",
    "\n",
    "    for train_index, test_index in skf.split(converted_X, y):\n",
    "\n",
    "        X_train, X_test = converted_X[train_index], converted_X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "        X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42) #approximate train, test, eval size (8500,) (1181,) (2126,)\n",
    "\n",
    "        #Data processing: Tokenization\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features)\n",
    "        tokenizer.fit_on_texts(X_train) #fitting the tokenizer on the train data\n",
    "        X_train_tokenized_padded = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen = maxlen)\n",
    "        X_test_tokenized_padded = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen = maxlen)\n",
    "        X_eval_tokenized_padded = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(X_eval), maxlen = maxlen)\n",
    "\n",
    "        #pair making\n",
    "        X1_train, X2_train, y1_label_train, y2_label_train, y_simil_train = pair_generator(X_train_tokenized_padded, y_train)\n",
    "        X1_eval, X2_eval, y1_label_eval, y2_label_eval, y_simil_eval = pair_generator(X_eval_tokenized_padded, y_eval)\n",
    "\n",
    "        #make similar labels 1 and dissimilar labels 0\n",
    "        y_simil_train = np.where(y_simil_train == -1, 0, y_simil_train)\n",
    "        y_simil_eval = np.where(y_simil_eval == -1, 0, y_simil_eval)\n",
    "\n",
    "        with open('train_eval_test_data_'+str(fold_count)+'.pkl','wb') as f:\n",
    "            pickle.dump([X1_train, X2_train, y1_label_train, y2_label_train, y_simil_train, X1_eval, X2_eval, y1_label_eval, y2_label_eval, y_simil_eval, X_test_tokenized_padded, y_test],f)\n",
    "\n",
    "        final_model = total_model(maxlen, max_features,embedding_dim)\n",
    "        losses = {'Distance': contrastive_loss, 'accuracyoutput_a': cross_entropy_loss_defined, 'accuracyoutput_b': cross_entropy_loss_defined}\n",
    "    #     weights = {'Distance': distance_weight, 'accuracyoutput':class_weight}\n",
    "        metrices = {'Distance':similarity_accuracy, 'accuracyoutput_a': class_accuracy, 'accuracyoutput_b': class_accuracy}\n",
    "        final_model.compile(optimizer = tf.keras.optimizers.Adam(1e-4), loss = losses)#, metrics = metrices)\n",
    "        print(final_model.summary())\n",
    "\n",
    "        checkpoint_path = \"saved_model/SNN/\"+str(fold_count)+\"/cp.ckpt\"\n",
    "        checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "        # Create a callback that saves the model's weights\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         verbose=1)\n",
    "\n",
    "        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=3)\n",
    "        final_model.fit(x = [X1_train, X2_train], y = [y_simil_train, y1_label_train, y2_label_train], batch_size=batch_size, epochs= epochs,\n",
    "                       validation_data = ([X1_eval, X2_eval], [y_simil_eval, y1_label_eval, y2_label_eval]), verbose = 1, callbacks=[es, cp_callback])\n",
    "        predictions =  tf.keras.activations.sigmoid(final_model.predict([X_test_tokenized_padded, X_test_tokenized_padded])[-1])\n",
    "        \n",
    "        predictions = np.where(predictions>0.5, 1, -1)\n",
    "        print(classification_report(y_test, predictions))\n",
    "        cl = classification_report(y_test, predictions, output_dict =True)\n",
    "        for t in metrics:\n",
    "            macro_avg[t].append(cl['macro avg'][t])\n",
    "            weighted_avg[t].append(cl['weighted avg'][t])\n",
    "\n",
    "\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        fold_count += 1\n",
    "    \n",
    "print(macro_avg, weighted_avg)\n",
    "    \n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
